{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Adam Nurlign 7/2/2025\n",
        "\n",
        "Hello there! In this notebook I will be implementing a comprehensive Neural Network Deep Learning Model Framework\n",
        "in which you can build and customise your own neural networks which can be trained and evaluated on datasets of your choosing.\n",
        "\n",
        "There are many modules in Python such as PyTorch and Scikit-learn that give you access to machine learning frameworks and\n",
        "allow you to build your own models made up of layers. I thought i would be a good exercise to be able to implement some of these\n",
        "features from scratch. I hope you enjoy!\n",
        "\n",
        "Here are some current constraints to my Machine Learning modules:\n",
        "\n",
        "-Must have a linear, activation, linear, activation .... linear network structure\n",
        "-The activations must be sigmoid or relu\n",
        "-Can only perform stochastic gradient descent\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qzHs8cilx8qc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "d52fe85e-4cc7-493c-d592-51319beefb8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAdam Nurlign 7/2/2025\\n\\nHello there! In this notebook I will be implementing a comprehensive Neural Network Deep Learning Model Framework\\nin which you can build and customise your own neural networks which can be trained and evaluated on datasets of your choosing.\\n\\nThere are many modules in Python such as PyTorch and Scikit-learn that give you access to machine learning frameworks and \\nallow you to build your own models made up of layers. I thought i would be a good exercise to be able to implement some of these\\nfeatures from scratch. I hope you enjoy!\\n\\nHere are some current constraints to my Machine Learning modules:\\n\\n-Must have a linear, activation, linear, activation .... linear network structure\\n-The activations must be sigmoid or relu\\n-Can only perform stochastic gradient descent\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "YJB18aX9uo8e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Layer Superclass\n",
        "class Layer():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def apply(self,x):\n",
        "    pass"
      ],
      "metadata": {
        "id": "eoyXcFOBxhyA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer(Layer):\n",
        "  def __init__(self,in_dim,out_dim,loss=None):\n",
        "    super().__init__()\n",
        "    #We should have a matrix with the dimension out_dim by in_dim which draws each element from the normal distribution with mean 0 and variance 4/(in_dim+out_dim)\n",
        "    variance = 4/(in_dim+out_dim)\n",
        "    std_dev=np.sqrt(variance)\n",
        "    self.weights = np.random.normal(loc=0.0, scale=std_dev, size=(out_dim, in_dim))\n",
        "    self.bias=np.zeros((out_dim,1))\n",
        "    #storing the derivative of objective with respect to weight matrix\n",
        "    self.weightGrad=None\n",
        "    #storing the deravitive of objective with respect to bias vector\n",
        "    self.biasGrad=None\n",
        "    #storing the deravitive of objective with respect to this layers output (linear output z)\n",
        "    self.outputGrad=None\n",
        "\n",
        "    #storing the forward pass output value\n",
        "    self.outputValue=None\n",
        "\n",
        "\n",
        "  def apply(self,x):\n",
        "    return self.weights@x+self.bias"
      ],
      "metadata": {
        "id": "VhPA8VYwwX5x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class reLU(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.outputValue=None\n",
        "  def apply(self,x):\n",
        "    return np.maximum(x,0)"
      ],
      "metadata": {
        "id": "4g151mOVwlqK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.outputValue=None\n",
        "  def apply(self,x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "SSGu47IqUFJ2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derivOfActWrtLinearInput(activationType,outputVal):\n",
        "  if (activationType==\"sigmoid\"):\n",
        "    #The deravitive of objective with respect to linear output depends on derivative of activation with respect to linear output which depends on the activation function\n",
        "    #For the sigmoid function the deravitive with respect to input = input * (1-input)\n",
        "    fi=np.copy(outputVal)\n",
        "    oneMinusfi=np.ones(fi.shape)-fi\n",
        "    derivOfAct=fi*oneMinusfi #this is the deravitive of activation with respect to linera input\n",
        "    derivOfActMatrix=np.diag(derivOfAct) #has DixDi dimensions where Di is the dimension of input into linear layer\n",
        "    return derivOfActMatrix\n",
        "  elif (activationType==\"relu\"):\n",
        "    #derivative of relu activation function with respect to input (linear output) is a diagonal matrix of 1's in the diagonal\n",
        "    #entries where the linear output is >0 and 0 elsewhere\n",
        "    fi=np.copy(outputVal)\n",
        "    OnesAndZeroes=(fi>0).astype(int)\n",
        "    OnesAndZeroes=OnesAndZeroes.flatten()\n",
        "    OnesAndZeroesMatrix=np.diag(OnesAndZeroes)\n",
        "    return OnesAndZeroesMatrix\n",
        "  else:\n",
        "    pass"
      ],
      "metadata": {
        "id": "4pEjJipL28gI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TvlHH_OJrIPi"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self,layers):\n",
        "    self.layers=layers\n",
        "\n",
        "  def predict(self,x,storeValues=False):\n",
        "    val=np.copy(x)\n",
        "    if (val.ndim==1):\n",
        "      val=val.reshape((val.shape[0],1))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      val=layer.apply(val)\n",
        "      if (storeValues==True):\n",
        "        layer.outputValue=val\n",
        "    return val\n",
        "\n",
        "  #This will specifically use sgd to train the model. The steps of gradient descent in general is to perform\n",
        "  #the forward pass, then backward pass #1 to obtain the gradient of objective with respect to each linear output, then\n",
        "  #the backward pass #2 to obtain the gradient of objective with respect to each parameter\n",
        "  def train_sgd(self,train_x,train_y,num_epochs,learning_rate,batch_size=1):\n",
        "    for epoch in range(num_epochs):\n",
        "      perm = np.random.permutation(train_x.shape[0])\n",
        "      # Apply permutation to both x and y\n",
        "      train_x_shuffled = train_x[perm]\n",
        "      train_y_shuffled = train_y[perm]\n",
        "\n",
        "      for i in range(len(train_x_shuffled)):\n",
        "        x=train_x_shuffled[i].reshape(-1,1)\n",
        "        y=train_y_shuffled[i].reshape(-1,1)\n",
        "        #The following is the forward pass\n",
        "        yHat=self.predict(x,storeValues=True)\n",
        "        #While we perform the forward pass in each layer we store the output of the layer so we have access to all the intermediate values\n",
        "        #Now we will perform the backward pass #1 to get the gradient of objective with respect to each linear output\n",
        "\n",
        "        #backward pass #1\n",
        "        #starting from the output layer and working backwards through only the linear layers. This assumes a linear-activation, linear-activation...\n",
        "        #structure to the nueral networks\n",
        "\n",
        "        #should only be iterating through linear layers assuming structure and skipping activation layers\n",
        "        for i in range(len(self.layers)-1,-1,-2):\n",
        "          layer=self.layers[i]\n",
        "          #if we are dealing with the last layer=output layer\n",
        "          if (i==len(self.layers)-1):\n",
        "            #Assume the loss function is squared error.\n",
        "            self.layers[i].outputGrad=2*(yHat-y)\n",
        "            continue\n",
        "          else:\n",
        "            term1=None\n",
        "            if isinstance(self.layers[i+1], Sigmoid):\n",
        "              term1=derivOfActWrtLinearInput(\"sigmoid\",layer.outputValue.flatten())\n",
        "            elif isinstance(self.layers[i+1],reLU):\n",
        "              term1=derivOfActWrtLinearInput(\"relu\",layer.outputValue.flatten())\n",
        "            else:\n",
        "              pass\n",
        "\n",
        "            #obtaining the transpose of weight matrix of next linear layer\n",
        "            term2=np.transpose(self.layers[i+2].weights)\n",
        "\n",
        "            #this is the gradient of objective with respect to next linear layers output which we calucated in the previous iteration of this backward pass #1\n",
        "            term3=self.layers[i+2].outputGrad\n",
        "            self.layers[i].outputGrad=term1@term2@term3\n",
        "\n",
        "      #backward pass #2\n",
        "      #now we calculate the gradient of objective with respect to weights and biases in linear layers\n",
        "        for i in range(len(self.layers)-1,-1,-2):\n",
        "          layer=self.layers[i]\n",
        "          derivObjWrtBias=layer.outputGrad\n",
        "          layer.bias=layer.bias-learning_rate*derivObjWrtBias\n",
        "\n",
        "          # Special case for the first linear layer\n",
        "          derivObjWrtWeights=None\n",
        "          if i==0:\n",
        "              derivObjWrtWeights=layer.outputGrad@np.transpose(x)\n",
        "          else:\n",
        "              derivObjWrtWeights=layer.outputGrad@np.transpose(self.layers[i-1].outputValue)\n",
        "\n",
        "          layer.weights=layer.weights-learning_rate*derivObjWrtWeights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def squared_error_loss(yHat,y):\n",
        "      #in Mathematics the squared error loss function is the sum of the squares of the difference between the two vectors\n",
        "\n",
        "      difference=yHat-y\n",
        "      return (np.transpose(difference))@difference"
      ],
      "metadata": {
        "id": "M8Qt8EmbL953"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def absolute_error_loss(yHat,y):\n",
        "  difference=yHat-y\n",
        "  error_vector=np.abs(difference)\n",
        "  return np.sum(error_vector)"
      ],
      "metadata": {
        "id": "dUpuPRgt68Y2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading and Preparing the Dataset\n",
        "\n",
        "data=np.loadtxt(\"sample_data/concrete.csv\",delimiter=\",\",skiprows=1)\n",
        "np.random.shuffle(data)\n",
        "splitIndex=int(0.8*len(data))\n",
        "ConcreteStrengthX=data[:splitIndex,:-1]\n",
        "ConcreteStrengthY=data[:splitIndex,-1].reshape(-1,1)\n",
        "print(ConcreteStrengthX.shape)\n",
        "print(ConcreteStrengthY.shape)\n",
        "ConcreteStrengthXTest=data[splitIndex:,:-1]\n",
        "ConcreteStrengthYTest=data[splitIndex:,-1].reshape(-1,1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CdZHlAhTtAO",
        "outputId": "41c18857-035e-49b3-99c8-1282653d2090"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(824, 8)\n",
            "(824, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(data):\n",
        "    mean = np.mean(data, axis=0)\n",
        "    std = np.std(data, axis=0)\n",
        "    return (data - mean) / std, mean, std\n",
        "\n",
        "# Standardize training set and store stats\n",
        "ConcreteStrengthX, train_mean, train_std = standardize_data(ConcreteStrengthX)\n",
        "\n",
        "# Standardize test set using **training mean and std**\n",
        "WConcreteStrengthXTest = (ConcreteStrengthXTest - train_mean) / train_std\n"
      ],
      "metadata": {
        "id": "82kdDGUVLHM4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "These will be tests for SGD on linear, act, linear, act, linear, act, linear (output) network architecture that was spelled out by the textbook\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ListOfLayers=[LinearLayer(8,3),reLU(),LinearLayer(3,3),reLU(),LinearLayer(3,3),reLU(),LinearLayer(3,1)]\n",
        "\n",
        "network=NeuralNetwork(ListOfLayers)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h-ZnOvMnty_V"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network.train_sgd(ConcreteStrengthX,ConcreteStrengthY,100,0.02)\n"
      ],
      "metadata": {
        "id": "CJkEOrTgMKUH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I am giving a 8x1 vector as expected to pass through the neural network. I should literally get a continous value spit out\n",
        "print(network.predict(np.array([1,1,1,1,1,1,1,1])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF3Z6QeCtLtO",
        "outputId": "f39fc57c-00a4-4983-9187-f26b3badbf44"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[35.11938887]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yHat=np.transpose(np.apply_along_axis(network.predict,0,(np.transpose(ConcreteStrengthXTest))))\n",
        "yHat=yHat.flatten()\n",
        "yHat=yHat.reshape(yHat.shape[0],1)\n",
        "\n",
        "#Number of datapoints in the validation dataset\n",
        "num_points=yHat.shape[0]\n",
        "print(\"The number of validation data points is: \"+str(num_points))\n",
        "\n",
        "se_loss_array=squared_error_loss(yHat,ConcreteStrengthYTest)\n",
        "\n",
        "print(se_loss_array)\n",
        "\n",
        "se_loss=float(se_loss_array)\n",
        "\n",
        "mse_loss=se_loss/num_points\n",
        "\n",
        "\n",
        "ae_loss=absolute_error_loss(yHat,ConcreteStrengthYTest)\n",
        "mae_loss=ae_loss/num_points\n",
        "\n",
        "\n",
        "print(\"Squared error on the validation dataset: \" + str(se_loss))\n",
        "print(\"Mean squared error on the validation dataset: \"+ str(mse_loss))\n",
        "print(\"Absolute error on the validation dataset: \" + str(ae_loss))\n",
        "print(\"Mean absolute error on the validation dataset: \"+ str(mae_loss))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "76RjnXtPMKaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11947a4-859e-4fd0-a51b-b8d2a0d19c0e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of validation data points is: 206\n",
            "[[48644.42505161]]\n",
            "Squared error on the validation dataset: 48644.42505161435\n",
            "Mean squared error on the validation dataset: 236.1379856874483\n",
            "Absolute error on the validation dataset: 2526.6753329282474\n",
            "Mean absolute error on the validation dataset: 12.265414237515763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-27-74747042.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  se_loss=float(se_loss_array)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3Cwn0_6wKY8G"
      }
    }
  ]
}