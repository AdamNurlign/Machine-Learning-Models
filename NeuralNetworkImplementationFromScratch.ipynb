{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Adam Nurlign 7/2/2025\n",
        "\n",
        "Hello there! In this notebook I will be implementing a comprehensive Neural Network Deep Learning Model Framework\n",
        "in which you can build and customise your own neural networks which can be trained and evaluated on datasets of your choosing.\n",
        "\n",
        "There are many modules in Python such as PyTorch and Scikit-learn that give you access to machine learning frameworks and\n",
        "allow you to build your own models made up of layers. I thought i would be a good exercise to be able to implement some of these\n",
        "features from scratch. I hope you enjoy!\n",
        "\n",
        "Here are some current constraints to my Machine Learning modules:\n",
        "\n",
        "-Must have a linear, activation, linear, activation .... linear network structure\n",
        "-The activations must be sigmoid or relu\n",
        "-Can only perform stochastic gradient descent\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qzHs8cilx8qc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "16cf7047-16d2-438a-928f-0b07acf5cc1a"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAdam Nurlign 7/2/2025\\n\\nHello there! In this notebook I will be implementing a comprehensive Neural Network Deep Learning Model Framework\\nin which you can build and customise your own neural networks which can be trained and evaluated on datasets of your choosing.\\n\\nThere are many modules in Python such as PyTorch and Scikit-learn that give you access to machine learning frameworks and\\nallow you to build your own models made up of layers. I thought i would be a good exercise to be able to implement some of these\\nfeatures from scratch. I hope you enjoy!\\n\\nHere are some current constraints to my Machine Learning modules:\\n\\n-Must have a linear, activation, linear, activation .... linear network structure\\n-The activations must be sigmoid or relu\\n-Can only perform stochastic gradient descent\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "YJB18aX9uo8e"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Layer Superclass\n",
        "class Layer():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def apply(self,x):\n",
        "    pass"
      ],
      "metadata": {
        "id": "eoyXcFOBxhyA"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer(Layer):\n",
        "  def __init__(self,in_dim,out_dim,activation=None,loss=None):\n",
        "    super().__init__()\n",
        "    #We should have a matrix with the dimension out_dim by in_dim which draws each element from the normal distribution with mean 0 and variance 4/(in_dim+out_dim)\n",
        "    variance = 4/(in_dim+out_dim)\n",
        "    std_dev=np.sqrt(variance)\n",
        "    self.weights = np.random.normal(loc=0.0, scale=std_dev, size=(out_dim, in_dim))\n",
        "    self.bias=np.zeros((out_dim,1))\n",
        "    #storing the derivative of objective with respect to weight matrix\n",
        "    self.weightGrad=None\n",
        "    #storing the deravitive of objective with respect to bias vector\n",
        "    self.biasGrad=None\n",
        "    #storing the deravitive of objective with respect to this layers output (linear output z)\n",
        "    self.outputGrad=None\n",
        "\n",
        "    #storing the forward pass output value\n",
        "    self.outputValue=None\n",
        "\n",
        "    self.activation=None\n",
        "    self.loss=loss\n",
        "\n",
        "    if (activation==\"sigmoid\"):\n",
        "      self.activation=Sigmoid()\n",
        "    elif (activation==\"relu\"):\n",
        "      self.activation=reLU()\n",
        "    elif (activation==\"softmax\"):\n",
        "      self.activation=Softmax()\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "\n",
        "  def apply(self,x):\n",
        "    return self.weights@x+self.bias"
      ],
      "metadata": {
        "id": "VhPA8VYwwX5x"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class reLU(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.outputValue=None\n",
        "  def apply(self,x):\n",
        "    return np.maximum(x,0)"
      ],
      "metadata": {
        "id": "4g151mOVwlqK"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.outputValue=None\n",
        "  def apply(self,x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "SSGu47IqUFJ2"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.outputValue=None\n",
        "  def apply(self, x):\n",
        "    x=x.flatten()\n",
        "    shift_x=x-np.max(x)\n",
        "    exponent=np.exp(shift_x)\n",
        "    denominator=np.sum(exponent)\n",
        "    answer=exponent/denominator\n",
        "    return answer.reshape(-1,1)\n",
        "\n"
      ],
      "metadata": {
        "id": "5rne9TDpBH0B"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derivOfActWrtLinearInput(activationType,outputVal):\n",
        "  if (activationType==\"sigmoid\"):\n",
        "    #The deravitive of objective with respect to linear output depends on derivative of activation with respect to linear output which depends on the activation function\n",
        "    #For the sigmoid function the deravitive with respect to input = input * (1-input)\n",
        "    fi=np.copy(outputVal)\n",
        "    oneMinusfi=np.ones(fi.shape)-fi\n",
        "    derivOfAct=fi*oneMinusfi #this is the deravitive of activation with respect to linera input\n",
        "    derivOfActMatrix=np.diag(derivOfAct) #has DixDi dimensions where Di is the dimension of input into linear layer\n",
        "    return derivOfActMatrix\n",
        "  elif (activationType==\"relu\"):\n",
        "    #derivative of relu activation function with respect to input (linear output) is a diagonal matrix of 1's in the diagonal\n",
        "    #entries where the linear output is >0 and 0 elsewhere\n",
        "    fi=np.copy(outputVal)\n",
        "    OnesAndZeroes=(fi>0).astype(int)\n",
        "    OnesAndZeroes=OnesAndZeroes.flatten()\n",
        "    OnesAndZeroesMatrix=np.diag(OnesAndZeroes)\n",
        "    return OnesAndZeroesMatrix\n",
        "  elif (activationType==\"softmax\"):\n",
        "    #Note here unlike the sigmoid and relu activation functions output val will be the activation layers output not linear\n",
        "    h=outputVal.reshape(-1,1)\n",
        "    #The derivative of softmax activation with respect to linear input is very simple mathematically, and this is a elegant,\n",
        "    #complicated expression which does the following operation: on the diagonals i of the matrix returned do hi*1-hi.\n",
        "    #On the off diagonal representing the deravitive of activation component i with respect to linear input component j do\n",
        "    #-hi*hj. The matrix we return is symmetric meaning its transpose equals itself so we don't have to worry about whether\n",
        "    #it is in denominator or numerator format.\n",
        "    return np.diagflat(h)-np.dot(h,h.T)\n",
        "  else:\n",
        "    pass"
      ],
      "metadata": {
        "id": "4pEjJipL28gI"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "TvlHH_OJrIPi"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self,layers):\n",
        "    self.layers=layers\n",
        "\n",
        "  def predict(self,x,storeValues=False):\n",
        "    val=np.copy(x)\n",
        "    if (val.ndim==1):\n",
        "      val=val.reshape((val.shape[0],1))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      val=layer.apply(val)\n",
        "      if (storeValues==True):\n",
        "        layer.outputValue=val\n",
        "      if (layer.activation!=None):\n",
        "        activationLayer=layer.activation\n",
        "        val=activationLayer.apply(val)\n",
        "        if (storeValues==True):\n",
        "          activationLayer.outputValue=val\n",
        "    return val\n",
        "\n",
        "  #This will specifically use sgd to train the model. The steps of gradient descent in general is to perform\n",
        "  #the forward pass, then backward pass #1 to obtain the gradient of objective with respect to each linear output, then\n",
        "  #the backward pass #2 to obtain the gradient of objective with respect to each parameter\n",
        "  def train_sgd(self,train_x,train_y,num_epochs,learning_rate,batch_size=1):\n",
        "    for epoch in range(num_epochs):\n",
        "      perm = np.random.permutation(train_x.shape[0])\n",
        "      # Apply permutation to both x and y\n",
        "      train_x_shuffled = train_x[perm]\n",
        "      train_y_shuffled = train_y[perm]\n",
        "\n",
        "      for i in range(len(train_x_shuffled)):\n",
        "        x=train_x_shuffled[i].reshape(-1,1)\n",
        "        y=train_y_shuffled[i].reshape(-1,1)\n",
        "        #The following is the forward pass\n",
        "        yHat=self.predict(x,storeValues=True)\n",
        "        #While we perform the forward pass in each layer we store the output of the layer so we have access to all the intermediate values\n",
        "        #Now we will perform the backward pass #1 to get the gradient of objective with respect to each linear output\n",
        "\n",
        "        #backward pass #1\n",
        "\n",
        "        for i in range(len(self.layers)-1,-1,-1):\n",
        "          layer=self.layers[i]\n",
        "          #if we are dealing with the last layer=output layer. For here dj/dz=da/dz*dJ/da\n",
        "          if (i==len(self.layers)-1):\n",
        "            activationType=None\n",
        "            if isinstance(layer.activation, Sigmoid):\n",
        "              activationType=\"sigmoid\"\n",
        "            elif isinstance(layer.activation, reLU):\n",
        "              activationType=\"relu\"\n",
        "            elif isinstance(layer.activation, Softmax):\n",
        "              activationType=\"softmax\"\n",
        "            else:\n",
        "              pass\n",
        "\n",
        "\n",
        "            if (layer.loss==\"se\"):\n",
        "              if (activationType==None):\n",
        "                layer.outputGrad=2*(yHat-y)\n",
        "              else:\n",
        "                djda=2*(yHat-y)\n",
        "                dadz=derivOfActWrtLinearInput(activationType,yHat)\n",
        "                layer.outputGrad=dadz@djda\n",
        "\n",
        "            elif (layer.loss==\"ce\"):\n",
        "\n",
        "              if (activationType==\"softmax\"):\n",
        "                #Very special case\n",
        "                self.layers[i].outputGrad=yHat-y\n",
        "\n",
        "              else:\n",
        "                deriv=np.zeros_like(yHat)\n",
        "                correctClassIndex=np.argmax(y)\n",
        "                probCorrectClass=yHat[correctClassIndex,0]\n",
        "                deriv[correctClassIndex,0]=-1*(1/(probCorrectClass))\n",
        "                djda=deriv\n",
        "                dadz=derivOfActWrtLinearInput(activationType,yHat)\n",
        "                layer.outputGrad=dadz@djda\n",
        "\n",
        "            continue\n",
        "\n",
        "          else:\n",
        "            term1=None\n",
        "            if (layer.activation==None):\n",
        "              term1=np.eye(layer.outputValue.shape[0])\n",
        "\n",
        "            elif isinstance(layer.activation, Sigmoid):\n",
        "              term1=derivOfActWrtLinearInput(\"sigmoid\",layer.outputValue.flatten())\n",
        "            elif isinstance(layer.activation,reLU):\n",
        "              term1=derivOfActWrtLinearInput(\"relu\",layer.outputValue.flatten())\n",
        "            elif isinstance(layer.activation,Softmax):\n",
        "              term1=derivOfActWrtLinearInput(\"softmax\",layer.activation.outputValue.flatten())\n",
        "            else:\n",
        "              pass\n",
        "\n",
        "            #obtaining the transpose of weight matrix of next linear layer\n",
        "            term2=np.transpose(self.layers[i+1].weights)\n",
        "\n",
        "            #this is the gradient of objective with respect to next linear layers output which we calucated in the previous iteration of this backward pass #1\n",
        "            term3=self.layers[i+1].outputGrad\n",
        "            self.layers[i].outputGrad=term1@term2@term3\n",
        "\n",
        "      #backward pass #2\n",
        "      #now we calculate the gradient of objective with respect to weights and biases in linear layers\n",
        "        for i in range(len(self.layers)-1,-1,-1):\n",
        "          layer=self.layers[i]\n",
        "          derivObjWrtBias=layer.outputGrad\n",
        "          layer.bias=layer.bias-learning_rate*derivObjWrtBias\n",
        "\n",
        "          # Special case for the first linear layer\n",
        "          derivObjWrtWeights=None\n",
        "          if i==0:\n",
        "              derivObjWrtWeights=layer.outputGrad@np.transpose(x)\n",
        "          else:\n",
        "              derivObjWrtWeights=layer.outputGrad@np.transpose(self.layers[i-1].activation.outputValue)\n",
        "\n",
        "          layer.weights=layer.weights-learning_rate*derivObjWrtWeights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def squared_error_loss(yHat,y):\n",
        "      #in Mathematics the squared error loss function is the sum of the squares of the difference between the two vectors\n",
        "\n",
        "      difference=yHat-y\n",
        "      return (np.transpose(difference))@difference"
      ],
      "metadata": {
        "id": "M8Qt8EmbL953"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def absolute_error_loss(yHat,y):\n",
        "  difference=yHat-y\n",
        "  error_vector=np.abs(difference)\n",
        "  return np.sum(error_vector)"
      ],
      "metadata": {
        "id": "dUpuPRgt68Y2"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(yHat,y):\n",
        "  #Only one component of y will be 1 and the rest will be 0. Therefore to find the index i  where yi =1 we can take the argmax\n",
        "  #this will be a column vector of the class of each datapoint\n",
        "  oneIndex=np.argmax(y,axis=1)\n",
        "\n",
        "  #this will be a column vector of the prob we predicted for the correct class of each datapoint\n",
        "  probCorrectClass = yHat[np.arange(yHat.shape[0]), oneIndex]\n",
        "\n",
        "  #vector of losses\n",
        "  individualLosses=-1*np.log(probCorrectClass)\n",
        "\n",
        "  return np.sum(individualLosses)\n"
      ],
      "metadata": {
        "id": "o4wxNLQCUgg9"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerize_output(Y):\n",
        "  #This function is going to take an output column of strings (classes) and convert it into an output column of numbers from 0 to k-1\n",
        "  Y_flat=Y.flatten()\n",
        "  num_points=Y_flat.shape[0]\n",
        "  #this is a 1d numpy array\n",
        "  unique_elems=np.unique(Y_flat)\n",
        "  unique_elems_list=list(unique_elems)\n",
        "  num_unique_elems=len(unique_elems_list)\n",
        "  k=num_unique_elems\n",
        "  unique_elems_list_enum=enumerate(unique_elems_list)\n",
        "\n",
        "\n",
        "  class_string_to_int_dict={string_class: int_class for (int_class,string_class) in unique_elems_list_enum}\n",
        "\n",
        "  def string_to_int_class(string):\n",
        "    #gets you the integer class\n",
        "    return class_string_to_int_dict[string]\n",
        "\n",
        "  convert_strings_to_ints=np.vectorize(string_to_int_class)\n",
        "\n",
        "  y_numbers=convert_strings_to_ints(Y_flat)\n",
        "\n",
        "\n",
        "  #To get it back into a column vector\n",
        "  #y_numbers=y_numbers.reshape(-1,1) # Remove this line\n",
        "  return y_numbers,k"
      ],
      "metadata": {
        "id": "G8250uQSn-zI"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_classification_y_dataset(Y_train,Y_test,k):\n",
        "  # Y_train and Y_test are going to have the dimensions of some nx1, we want to turn this into nxk where k is the number of classes, the classes go from 0 to k-1\n",
        "  Y_train_flat=Y_train.flatten()\n",
        "  Y_test_flat=Y_test.flatten()\n",
        "\n",
        "  num_train_points=Y_train_flat.shape[0]\n",
        "  num_test_points=Y_test_flat.shape[0]\n",
        "\n",
        "  transformed_y_train=np.zeros((num_train_points,k))\n",
        "  transformed_y_test=np.zeros((num_test_points,k))\n",
        "\n",
        "  transformed_y_train[np.arange(num_train_points),Y_train_flat]=1\n",
        "  transformed_y_test[np.arange(num_test_points),Y_test_flat]=1\n",
        "\n",
        "  return transformed_y_train,transformed_y_test\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HbWMvVnMmg4B"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractDatasetComponentsClassification(data):\n",
        "  np.random.shuffle(data)\n",
        "  splitIndex=int(0.8*len(data))\n",
        "\n",
        "  #I must numerise the output class column over here\n",
        "\n",
        "  Y=data[:,-1]\n",
        "  #y is nx1\n",
        "  data[:,-1],k=numerize_output(Y)\n",
        "\n",
        "  X_train_data=data[:splitIndex,:-1]\n",
        "  Y_train_data=data[:splitIndex,-1].reshape(-1,1).astype(int)\n",
        "\n",
        "  X_test_data=data[splitIndex:,:-1]\n",
        "  Y_test_data=data[splitIndex:,-1].reshape(-1,1).astype(int)\n",
        "\n",
        "  Y_train_data,Y_test_data=transform_classification_y_dataset(Y_train_data,Y_test_data,k)\n",
        "\n",
        "  return X_train_data,Y_train_data,X_test_data,Y_test_data\n",
        "\n"
      ],
      "metadata": {
        "id": "8nrtwIrBm6s_"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractDatasetComponentsRegression(data):\n",
        "  np.random.shuffle(data)\n",
        "  splitIndex=int(0.8*len(data))\n",
        "  X_train_data=data[:splitIndex,:-1]\n",
        "  Y_train_data=data[:splitIndex,-1].reshape(-1,1)\n",
        "  X_test_data=data[splitIndex:,:-1]\n",
        "  Y_test_data=data[splitIndex:,-1].reshape(-1,1)\n",
        "  return X_train_data,Y_train_data,X_test_data,Y_test_data\n",
        "\n"
      ],
      "metadata": {
        "id": "zTQaOXwqLzG9"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading and preparing the regression dataset\n",
        "concrete_df=pd.read_csv(\"sample_data/concrete.csv\")\n",
        "concrete_data=concrete_df.to_numpy()\n",
        "ConcreteStrengthX,ConcreteStrengthY,ConcreteStrengthXTest,ConcreteStrengthYTest=extractDatasetComponentsRegression(concrete_data)\n",
        "\n",
        "#Loading and preparing the classification dataset\n",
        "iris_df=pd.read_csv(\"sample_data/iris.csv\")\n",
        "iris_data=iris_df.to_numpy()\n",
        "iris_data=iris_data[1:,:]\n",
        "IrisClassX,IrisClassY,IrisClassXTest,IrisClassYTest=extractDatasetComponentsClassification(iris_data)\n",
        "IrisClassX=IrisClassX.astype(float)\n",
        "IrisClassY=IrisClassY.astype(float)\n",
        "IrisClassXTest=IrisClassXTest.astype(float)\n",
        "IrisClassYTest=IrisClassYTest.astype(float)\n",
        "\n"
      ],
      "metadata": {
        "id": "0CdZHlAhTtAO"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(data):\n",
        "    mean = np.mean(data,axis=0)\n",
        "    std = np.std(data,axis=0)\n",
        "    return (data-mean) /std, mean, std\n",
        "\n",
        "ConcreteStrengthX, concrete_train_mean, concrete_train_std = standardize_data(ConcreteStrengthX)\n",
        "ConcreteStrengthXTest=(ConcreteStrengthXTest - concrete_train_mean) / concrete_train_std\n",
        "\n",
        "IrisClassX, iris_train_mean, iris_train_std=standardize_data(IrisClassX)\n",
        "IrisClassXTest=(IrisClassXTest-iris_train_mean)/iris_train_std\n",
        "\n"
      ],
      "metadata": {
        "id": "82kdDGUVLHM4"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "These will be tests for SGD on linear, act, linear, act, linear, act, linear (output) network architecture that was spelled out by the textbook\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ListOfLayersConcreteRegression=[LinearLayer(8,3,activation=\"relu\"),LinearLayer(3,3,activation=\"relu\"),LinearLayer(3,1,loss=\"se\")]\n",
        "ConcreteNetwork=NeuralNetwork(ListOfLayersConcreteRegression)\n",
        "\n",
        "\n",
        "ListOfLayersIrisClassification=[LinearLayer(4,3,activation=\"relu\"),LinearLayer(3,3,activation=\"relu\"),LinearLayer(3,3,activation=\"softmax\",loss=\"ce\")]\n",
        "IrisNetwork=NeuralNetwork(ListOfLayersIrisClassification)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h-ZnOvMnty_V"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConcreteNetwork.train_sgd(ConcreteStrengthX,ConcreteStrengthY,100,0.02)\n",
        "\n",
        "IrisNetwork.train_sgd(IrisClassX,IrisClassY,100,0.02)\n",
        "\n"
      ],
      "metadata": {
        "id": "CJkEOrTgMKUH"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I am giving a 8x1 vector as expected to pass through the neural network. I should literally get a continous value spit out\n",
        "print(ConcreteNetwork.predict(np.array([540,0,0,162,2.5,1040,676,28])))\n",
        "\n",
        "probabilities=IrisNetwork.predict(np.array([6.1,3.0,4.6,1.4]))\n",
        "#Note that for this output the first component is the probability of belonging to Iris-setosa class, then Iris-versicolor class,\n",
        "#then Iris-virginica class\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF3Z6QeCtLtO",
        "outputId": "eae12908-fee8-437e-88d8-45b45bbea195"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[37.01465212]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes=[\"iris-setosa\",\"iris-versicolor\",\"iris-virginca\"]\n",
        "for i in range(len(classes)):\n",
        "  print(\"Probability of class \"+str(i)+\" \"+classes[i]+ \" is \"+str(probabilities[i][0]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mss9UbOr1yvS",
        "outputId": "20876a29-1c3a-4753-d997-f8b164f4b12e"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of class 0 iris-setosa is 1.31488538050364e-37\n",
            "Probability of class 1 iris-versicolor is 1.976022586822745e-26\n",
            "Probability of class 2 iris-virginca is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yHatConcrete=np.transpose(np.apply_along_axis(ConcreteNetwork.predict,0,(np.transpose(ConcreteStrengthXTest))))\n",
        "yHatConcrete=yHatConcrete.flatten()\n",
        "yHatConcrete=yHatConcrete.reshape(yHatConcrete.shape[0],1)\n",
        "\n",
        "#Number of datapoints in the validation dataset\n",
        "num_points=yHatConcrete.shape[0]\n",
        "print(\"The number of validation data points is: \"+str(num_points))\n",
        "\n",
        "se_loss_array=squared_error_loss(yHatConcrete,ConcreteStrengthYTest)\n",
        "\n",
        "se_loss=float(se_loss_array)\n",
        "\n",
        "mse_loss=se_loss/num_points\n",
        "\n",
        "\n",
        "ae_loss=absolute_error_loss(yHatConcrete,ConcreteStrengthYTest)\n",
        "mae_loss=ae_loss/num_points\n",
        "\n",
        "\n",
        "print(\"Squared error on the validation dataset: \" + str(se_loss))\n",
        "print(\"Mean squared error on the validation dataset: \"+ str(mse_loss))\n",
        "print(\"Absolute error on the validation dataset: \" + str(ae_loss))\n",
        "print(\"Mean absolute error on the validation dataset: \"+ str(mae_loss))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "76RjnXtPMKaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d27a071-0adf-4490-fd7a-67b53f4d56d4"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of validation data points is: 206\n",
            "Squared error on the validation dataset: 62116.17094120573\n",
            "Mean squared error on the validation dataset: 301.53481039420257\n",
            "Absolute error on the validation dataset: 2912.4516508846464\n",
            "Mean absolute error on the validation dataset: 14.138114810119642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3964664427.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  se_loss=float(se_loss_array)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "yHatIris=np.transpose(np.apply_along_axis(IrisNetwork.predict,0,(np.transpose(IrisClassXTest))))\n",
        "yHatIris=yHatIris.reshape(30,3)\n",
        "#This is nxd\n",
        "num_points=yHatIris.shape[0]\n",
        "print(\"The number of validation data points is: \"+str(num_points))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ce_loss_array=cross_entropy_loss(yHatIris,IrisClassYTest)\n",
        "\n",
        "\n",
        "\n",
        "ce_loss=float(ce_loss_array)\n",
        "\n",
        "print(\"Cross entropy loss on the validation dataset: \" + str(ce_loss))\n",
        "\n",
        "average_ce_loss=ce_loss/num_points\n",
        "print(\"The average cross entropy loss over the validation dataset: \"+str(average_ce_loss))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNcP7tFFSCpy",
        "outputId": "49620191-5b5d-46b3-c4aa-112abc043965"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of validation data points is: 30\n",
            "Cross entropy loss on the validation dataset: 8.432537572327579\n",
            "The average cross entropy loss over the validation dataset: 0.28108458574425266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3Cwn0_6wKY8G"
      }
    }
  ]
}