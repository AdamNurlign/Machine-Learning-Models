{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "LoLJ2YUTyKWg",
        "outputId": "b879f9ae-d0a8-4aa8-9b7f-fcd93610fd3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAdam Nurlign 7/1/2025\\n\\nHello there! In this notebook I will be implementing various Linear Regression Machine Learning\\nModels for predicting continous values from continous input features. There are many\\nmodules in Python such as PyTorch and Scikit-learn that give you access to linear regression models\\nbut I thought it would be a good exercise to be able to implement this from scratch without looking\\nat my notes. I hope you enjoy!\\n\\nThere are two algorithms I implement for optimising the parameters of my Linear Regression model: Closed-form solution and \\nGradient Descent.\\n\\nClarification: To be more specific I will be implementing Linear Regression with polynomial\\nfeature engineering which will in effect allow us to fit any degree polynomial to the training\\ndataset.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "\"\"\"\n",
        "Adam Nurlign 7/1/2025\n",
        "\n",
        "Hello there! In this notebook I will be implementing various Linear Regression Machine Learning\n",
        "Models for predicting continous values from continous input features. There are many\n",
        "modules in Python such as PyTorch and Scikit-learn that give you access to linear regression models\n",
        "but I thought it would be a good exercise to be able to implement this from scratch without looking\n",
        "at my notes. I hope you enjoy!\n",
        "\n",
        "There are two algorithms I implement for optimising the parameters of my Linear Regression model: Closed-form solution and\n",
        "Gradient Descent.\n",
        "\n",
        "Clarification: To be more specific I will be implementing Linear Regression with polynomial\n",
        "feature engineering which will in effect allow us to fit any degree polynomial to the training\n",
        "dataset.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "UxxCfSJ70FPJ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=np.loadtxt(\"sample_data/concrete.csv\",delimiter=\",\",skiprows=1)\n",
        "np.random.shuffle(data)\n",
        "splitIndex=int(0.8*len(data))\n",
        "ConcreteStrengthX=data[:splitIndex,:-1]\n",
        "ConcreteStrengthY=data[:splitIndex,-1].reshape(-1,1)\n",
        "print(ConcreteStrengthX.shape)\n",
        "print(ConcreteStrengthY.shape)\n",
        "ConcreteStrengthXTest=data[splitIndex:,:-1]\n",
        "ConcreteStrengthYTest=data[splitIndex:,-1].reshape(-1,1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUmu4IFIEQ33",
        "outputId": "9aed80e7-2d02-488f-9b8c-8e6566ac9cb1"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(824, 8)\n",
            "(824, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def standardize_data(data):\n",
        "    mean=np.mean(data,axis=0)\n",
        "    std=np.std(data,axis=0)\n",
        "    return (data-mean)/ std, mean, std\n",
        "\n",
        "# Standardize training set and store stats\n",
        "ConcreteStrengthX, train_mean, train_std = standardize_data(ConcreteStrengthX)\n",
        "\n",
        "# Standardize test set using **training mean and std**\n",
        "ConcreteStrengthXTest = (ConcreteStrengthXTest - train_mean) / train_std\n"
      ],
      "metadata": {
        "id": "hKfAlRFlYqcN"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makeDesignMatrix(xRaw,degree):\n",
        "  finalX=np.copy(xRaw)\n",
        "  for i in range(2,degree+1):\n",
        "    xPower=xRaw**degree\n",
        "    finalX=np.concatenate((finalX,xPower),axis=1)\n",
        "  return np.concatenate((np.ones((xRaw.shape[0],1)),finalX),axis=1)"
      ],
      "metadata": {
        "id": "-b4UOkU20rL_"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(y,yHat):\n",
        "  n=y.shape[0]\n",
        "  difference=y-yHat\n",
        "  differenceSquared=difference*difference\n",
        "  return (np.sum(differenceSquared.flatten()))/n"
      ],
      "metadata": {
        "id": "6skPDZvizY39"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MAE(y,yHat):\n",
        "  n=y.shape[0]\n",
        "  difference=y-yHat\n",
        "  absoluteDifference=np.abs(difference)\n",
        "  return (np.sum(absoluteDifference))/n\n",
        "\n"
      ],
      "metadata": {
        "id": "-UGpAsYUvQM9"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeBatches(X,y,batch_size):\n",
        "  n=X.shape[0]\n",
        "  num_batches=n//batch_size\n",
        "  listOfBatches=[]\n",
        "  for i in range(num_batches):\n",
        "    listOfBatches.append((X[i*batch_size:(i+1)*batch_size,:],y[i*batch_size:(i+1)*batch_size,:]))\n",
        "  if (n%batch_size!=0):\n",
        "    listOfBatches.append((X[num_batches*batch_size:,:],y[num_batches*batch_size:,:]))\n",
        "  return listOfBatches\n"
      ],
      "metadata": {
        "id": "kpHAraRyJOL5"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeGradient(X,y,w):\n",
        "  #The gradient value that we compute depends on the X and y data we feed (which depends on batch) and the paramter as well\n",
        "  n=X.shape[0]\n",
        "  Xt=np.transpose(X)\n",
        "  term1=(-1*Xt)@y\n",
        "  term2=Xt@X@w\n",
        "  return (2/n)*(term1+term2)"
      ],
      "metadata": {
        "id": "G14I12JLOyjJ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffleDataset(X, y):\n",
        "        idxs = np.arange(X.shape[0])\n",
        "        np.random.shuffle(idxs)\n",
        "        XShuffled, yShuffled= X[idxs, :], y[idxs, :]\n",
        "\n",
        "        return X[idxs, :], y[idxs, :]"
      ],
      "metadata": {
        "id": "aNKpsf64b-7t"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegModel():\n",
        "  def __init__(self):\n",
        "    self.params=None\n",
        "    self.degree=None\n",
        "    self.lr=None\n",
        "\n",
        "  def fit_closed_form(self,xRaw,trainY,degree):\n",
        "    self.degree=degree\n",
        "    X=makeDesignMatrix(xRaw,self.degree)\n",
        "    #Initializes the parameters- is not necessary for the closed form solution implementation\n",
        "    #but only for the gradient descent implementation\n",
        "\n",
        "    self.params=np.ones((X.shape[1],1))\n",
        "    Xt=np.transpose(X)\n",
        "    temp1=Xt@X\n",
        "    temp2=np.linalg.inv(temp1)\n",
        "    self.params=temp2@Xt@trainY\n",
        "\n",
        "\n",
        "  def fit_gradient_descent(self,xRaw,trainY,degree,num_epochs,batch_size,lr):\n",
        "    #We will implement training via minibatch gradient descent which is a generalization of\n",
        "    #stochastic gradient descent (batchsize=1) and gradient descent (batchsize=full)\n",
        "    self.degree=degree\n",
        "    self.lr=lr\n",
        "    X=makeDesignMatrix(xRaw,self.degree)\n",
        "    #Have to initialize paramaters\n",
        "    self.params=np.ones((X.shape[1],1))\n",
        "    for i in range(num_epochs):\n",
        "      shuffle_X,shuffle_y=shuffleDataset(X,trainY)\n",
        "      batchesList=computeBatches(shuffle_X,shuffle_y,batch_size)\n",
        "      for (XBatch,yBatch) in batchesList:\n",
        "        gradient=computeGradient(XBatch,yBatch,self.params)\n",
        "        self.params=self.params-self.lr*gradient\n",
        "\n",
        "\n",
        "\n",
        "  def predict(self,xRaw):\n",
        "    X=makeDesignMatrix(xRaw,self.degree)\n",
        "    return X@self.params\n",
        "\n",
        "  #mean squared error loss function\n",
        "  def computeMSE(self,xTestRaw,yTest):\n",
        "    yHat=self.predict(xTestRaw)\n",
        "    return MSE(yTest,yHat)\n",
        "\n",
        "  #mean absolute absolute error loss function\n",
        "  def computeMAE(self,xTestRaw,yTest):\n",
        "    yHat=self.predict(xTestRaw)\n",
        "    return MAE(yTest,yHat)"
      ],
      "metadata": {
        "id": "mdr_b_U9zJbZ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testModel1=LinearRegModel()\n",
        "testModel1.fit_closed_form(ConcreteStrengthX,ConcreteStrengthY,2)\n",
        "\n",
        "testModel2=LinearRegModel()\n",
        "testModel2.fit_gradient_descent(ConcreteStrengthX,ConcreteStrengthY,2,500,100,0.02)\n",
        "\n",
        "print(\"Here are the parameters for the Linear Regression Model that was trained using the closed-form solution:\")\n",
        "print(testModel1.params)\n",
        "print(\"----------------------\")\n",
        "print(\"Here are the parameters for the Linear Regression Model that was trained using the gradient-descent algorithm:\")\n",
        "print(testModel2.params)\n",
        "print(\"----------------------\")\n",
        "print(\"Here are the differences in the model's paramters: gradientDescentModel's parameters- closed-form solution model's parameters:\")\n",
        "print(testModel2.params-testModel1.params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYIPDEizyssB",
        "outputId": "8f4413ff-2bd9-4199-a208-1e6d5ca9aca4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the parameters for the Linear Regression Model that was trained using the closed-form solution:\n",
            "[[42.36735528]\n",
            " [12.47601188]\n",
            " [ 9.34543882]\n",
            " [ 4.11217839]\n",
            " [-3.43416244]\n",
            " [ 2.71519318]\n",
            " [ 1.03204326]\n",
            " [ 1.05825802]\n",
            " [17.61631408]\n",
            " [-0.16640916]\n",
            " [-0.97403573]\n",
            " [-0.97743201]\n",
            " [ 0.7799715 ]\n",
            " [-1.51583976]\n",
            " [ 0.32740372]\n",
            " [-1.06594398]\n",
            " [-3.26664753]]\n",
            "----------------------\n",
            "Here are the parameters for the Linear Regression Model that was trained using the gradient-descent algorithm:\n",
            "[[ 4.24839284e+01]\n",
            " [ 1.24674724e+01]\n",
            " [ 9.28203242e+00]\n",
            " [ 4.05618631e+00]\n",
            " [-3.53150438e+00]\n",
            " [ 2.58544175e+00]\n",
            " [ 1.01908141e+00]\n",
            " [ 1.10966289e+00]\n",
            " [ 1.76761257e+01]\n",
            " [ 4.21444080e-02]\n",
            " [-8.58659765e-01]\n",
            " [-9.22643545e-01]\n",
            " [ 8.53814141e-01]\n",
            " [-1.44183752e+00]\n",
            " [ 5.67553620e-01]\n",
            " [-1.14814882e+00]\n",
            " [-3.17008424e+00]]\n",
            "----------------------\n",
            "Here are the differences in the model's paramters: gradientDescentModel's parameters- closed-form solution model's parameters:\n",
            "[[ 0.11657313]\n",
            " [-0.00853949]\n",
            " [-0.0634064 ]\n",
            " [-0.05599208]\n",
            " [-0.09734195]\n",
            " [-0.12975144]\n",
            " [-0.01296185]\n",
            " [ 0.05140486]\n",
            " [ 0.05981163]\n",
            " [ 0.20855357]\n",
            " [ 0.11537597]\n",
            " [ 0.05478847]\n",
            " [ 0.07384264]\n",
            " [ 0.07400224]\n",
            " [ 0.2401499 ]\n",
            " [-0.08220484]\n",
            " [ 0.09656329]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we can see how both of our linear regression models (trained in the two ways) performs on the testing dataset\n",
        "#This uses the Mean Squared Error loss function\n",
        "print(\"MSE for closed-form solution model on validation dataset:\")\n",
        "print(testModel1.computeMSE(ConcreteStrengthXTest,ConcreteStrengthYTest))\n",
        "print(\"MSE for gradient descent model on validation dataset:\")\n",
        "print(testModel2.computeMSE(ConcreteStrengthXTest,ConcreteStrengthYTest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE8_QjBZc0aO",
        "outputId": "40a61230-cad2-4c5d-88cd-1364a1d79ca2"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for closed-form solution model on validation dataset:\n",
            "66.66735236167872\n",
            "MSE for gradient descent model on validation dataset:\n",
            "67.04971600813015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Same thing but this uses the Mean Absolute Error loss function\n",
        "print(\"MAE for closed-form solution model on validation dataset:\")\n",
        "print(testModel1.computeMAE(ConcreteStrengthXTest,ConcreteStrengthYTest))\n",
        "print(\"MAE for gradient descent model on validation dataset:\")\n",
        "print(testModel2.computeMAE(ConcreteStrengthXTest,ConcreteStrengthYTest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sKqS3z4vvDO",
        "outputId": "705b8f64-a1d6-48da-aa22-da89e2ac0cb0"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE for closed-form solution model on validation dataset:\n",
            "6.3927804450547665\n",
            "MAE for gradient descent model on validation dataset:\n",
            "6.413939376443549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HVde0E8lvuEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As a sanity check I want to see that my models make accurate predictions on the data that they trained on\n",
        "#This uses the Mean Squared Error loss function\n",
        "print(\"MSE for closed-form solution model on training dataset:\")\n",
        "print(testModel1.computeMSE(ConcreteStrengthX,ConcreteStrengthY))\n",
        "print(\"MSE for gradient descent model on training dataset:\")\n",
        "print(testModel2.computeMSE(ConcreteStrengthX,ConcreteStrengthY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4PHWlAViZFh",
        "outputId": "1690d459-0db6-4ff8-897b-f251fb728306"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for closed-form solution model on training dataset:\n",
            "62.28269156991075\n",
            "MSE for gradient descent model on training dataset:\n",
            "63.45772112080617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Same but this uses the Mean Absolute Error loss function\n",
        "print(\"MAE for closed-form solution model on training dataset:\")\n",
        "print(testModel1.computeMAE(ConcreteStrengthX,ConcreteStrengthY))\n",
        "print(\"MAE for gradient descent model on training dataset:\")\n",
        "print(testModel2.computeMAE(ConcreteStrengthX,ConcreteStrengthY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAixEeuAv-Cs",
        "outputId": "c6d38e81-5cc1-4b9e-cde1-c61f98adcc51"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE for closed-form solution model on training dataset:\n",
            "5.980865058610757\n",
            "MAE for gradient descent model on training dataset:\n",
            "6.046420206544249\n"
          ]
        }
      ]
    }
  ]
}